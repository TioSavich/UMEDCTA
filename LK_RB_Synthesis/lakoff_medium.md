The material inferences and cognitive algorithms described in *Where Mathematics Comes From*—that would serve as the content for such a system. Brandom's framework explains the *structure* of saying and doing, of asserting and inferring. Lakoff & Núñez's work provides the specific, embodied *content* of mathematical "doing" that gives rise to mathematical "saying."

The core of the synthesis is this: **Conceptual metaphors are the cognitive algorithms that generate the material inferences of mathematics.**

Below are the foundational material inferences and algorithms from *Where Mathematics Comes From*, framed in a way that prepares them for integration into a Brandomian normative-pragmatic framework.

***

### The Material Inferences and Cognitive Algorithms of Embodied Mathematics

These rules are presented in a hierarchical fashion, from the most basic bodily experiences to higher-level abstractions. In a Brandomian sense, each level makes explicit what is implicit in the level below it.

#### Level 1: The Foundational Logic of Embodied Experience (Image Schemas)

Before any arithmetic, there are basic spatial logics derived from our sensory-motor interaction with the world. These are pre-conceptual structures that make reasoning possible. The most crucial is the **Container Schema**.

**Algorithm 1: The Cognitive Algorithm of Containment**
*   **Input:** A perception or conception of a boundary, an interior, and an exterior.
*   **Procedure:** Structure the perception/conception according to the Container image schema.
*   **Output:** A set of material inferences based on the logic of containment.

**Material Inferences of the Container Schema:**
*   **Totality:** Everything is either inside a container or outside of it.
    *   *Brandomian interpretation:* For any object `x` and container `A`, one is committed to either `In(x, A)` or `Not In(x, A)`. This is the embodied grounding for the Law of the Excluded Middle.
*   **Transitivity of Inclusion:** If Container A is in Container B, and object X is in Container A, then X is in Container B.
    *   *Brandomian interpretation:* A commitment to `In(A, B)` and `In(x, A)` is, by virtue of the concepts involved, a commitment to `In(x, B)`. This grounds hypothetical syllogism.
*   **Modus Ponens:** If an object X is in container A, and A is a sub-region of B, then X is in B.
*   **Modus Tollens:** If an object Y is outside container B, and A is a sub-region of B, then Y is outside of A.

These are not formal logical rules; they are material inferences about space and containment that we know how to navigate implicitly because we have bodies that exist in space.

#### Level 2: The Grounding Metaphors for Basic Arithmetic (The "4Gs")

Arithmetic is not an abstract, disembodied system. It is grounded in everyday activities through a small set of foundational metaphors. The most basic of these is the **Arithmetic is Object Collection** metaphor.

**Algorithm 2: The Object Collection Metaphor**
*   **Source Domain:** The physical activity of collecting objects into piles.
*   **Target Domain:** The abstract domain of numbers and addition.
*   **Procedure (Mapping):**
    1.  Map collections of objects to numbers.
    2.  Map the size of the collection to the magnitude of the number.
    3.  Map the act of putting collections together to the operation of addition.
    4.  Map the act of taking a smaller collection from a larger one to subtraction.
*   **Output:** A set of material inferences about arithmetic, inherited from the logic of object manipulation.

**Material Inferences from the Object Collection Metaphor:**
*   **Closure:** Putting collections together results in a collection. → Addition results in a number.
*   **Commutativity:** Putting collection A with collection B results in a collection of the same size as putting B with A. → `a + b = b + a`.
*   **Associativity:** Adding collection A to (B+C) results in a collection of the same size as adding (A+B) to C. → `a + (b + c) = (a + b) + c`.
*   **Identity (Zero):** An empty collection is created metaphorically from the absence of objects. Adding an empty collection to another collection doesn't change its size. → `n + 0 = n`. This is the *Zero Collection Metaphor*.

These "laws" of arithmetic are not arbitrary axioms but are made intelligible and are grounded in the material practice of manipulating objects.

#### Level 3: Linking Metaphors and Further Abstractions

Different domains of mathematics are connected via linking metaphors, which export the inferential structure of one domain to another. The most crucial for extending arithmetic is **Numbers Are Points on a Line**.

**Algorithm 3: The Number Line Metaphor**
*   **Source Domain:** Motion and location along a path (Source-Path-Goal Schema).
*   **Target Domain:** The domain of numbers.
*   **Procedure (Mapping):**
    1.  Map points on a line to numbers.
    2.  Map the origin of the line to the number Zero.
    3.  Map distance from the origin to the magnitude of a number.
    4.  Map direction from the origin to positive/negative value.
    5.  Map order of points on the line to the ordering of numbers (`<` and `>`).
*   **Output:** A new set of material inferences about numbers based on the spatial logic of a line.

**Material Inferences from the Number Line Metaphor:**
*   **Total Ordering:** For any two distinct points on a line, one is always to the left of the other. → For any two distinct numbers `a` and `b`, either `a < b` or `b < a`.
*   **Transitivity of Order:** If point A is to the left of B and B is to the left of C, then A is to the left of C. → If `a < b` and `b < c`, then `a < c`.
*   **Density (for rationals/reals):** Between any two points on a line, there is another point. → Between any two rational/real numbers, there is another rational/real number.

#### Level 4: The Algorithm for Actual Infinity

Actual infinity (infinity as a thing) is not something we experience. It is a metaphorical construct, created by the **Basic Metaphor of Infinity (BMI)**. The BMI is a cognitive algorithm that conceptualizes an endless process as a completed process with a final, unique result.

**Algorithm 4: The Basic Metaphor of Infinity (BMI)**
*   **Input:** An unending, iterative process (e.g., counting, creating polygons with more sides, a sequence). This is "Potential Infinity."
*   **Procedure (Mapping from a Completed Process):**
    1.  **Initial State:** Identify the initial state of the unending process (e.g., the number 1, a triangle).
    2.  **Iterative Process:** Identify the process that gets from one state to the next (e.g., "add 1," "add a side").
    3.  **Metaphorical Completion:** Add to this ongoing process a final, "ultimate" state. This state is conceptualized as the result of the process having been completed. This is "Actual Infinity."
    4.  **Uniqueness:** This final state is entailed to be unique and to follow every non-final state.
*   **Output:** An "actual infinity"—a single, discrete entity (e.g., the set of all natural numbers, the point at infinity, a limit).

**Material Inferences from the BMI:**
*   **Existence of a Limit:** An infinite sequence that "approaches" a value has a final, unique point it reaches.
*   **Existence of Infinite Sets:** The unending process of generating numbers (1, 2, 3...) results in a completed, unique entity: the set of all natural numbers.
*   **Axiom of Induction:** What is true for the initial state (number 1) and is true for the process of getting from n to n+1, is metaphorically inferred to be true for the final "resultant state"—the entire set of natural numbers.

#### Level 5: The Metaphorical Nature of Axioms and Formal Systems

From this embodied perspective, formal systems like Boole's algebra are not direct descriptions of the "Laws of Thought" but are the result of a chain of metaphorical mappings that make the logic of an embodied schema explicit.

**Algorithm 5: The Formal Reduction of Logic (Boole's Algorithm)**
*   **Input:** The material inferences of the Container Schema.
*   **Procedure:**
    1.  Apply the **Classes are Containers** metaphor. This maps the spatial logic of containment onto the logic of classes (e.g., being inside a container becomes being a member of a class).
    2.  Apply **Boole's Metaphor**, which links the algebra of numbers (0 and 1) to the algebra of classes (the empty and universal classes). This metaphorically defines class operations (union, intersection) in terms of arithmetic-like operations.
    3.  Apply a **Symbolization mapping**, which associates the conceptual operations on classes with formal symbols (`∪`, `∩`, `¬`).
*   **Output:** A formal symbolic logic whose "truth" is grounded in the material inferences of the Container schema.

This synthesis reveals that the "laws" of mathematics are not transcendent truths but are **stable, material inferential norms** that arise from our embodiment and are systematically extended through powerful cognitive algorithms, chiefly conceptual metaphor. The next step in writing the code would be to implement these specific metaphorical mappings as procedures for generating new commitments and entitlements within the normative framework.

Of course. The initial set of algorithms and inferences lays the groundwork. To be comprehensive, we must now articulate the more complex metaphorical structures that Lakoff and Núñez identify as the foundation for the rest of mathematics, from algebra to calculus.

This is the next step in the synthesis: detailing the cognitive machinery that gives rise to the sophisticated content that Brandom's normative framework can then account for.

***

### The Material Inferences and Cognitive Algorithms of Embodied Mathematics (Continued)

#### Level 6: Extending the Number System — The Logic of Zero, Negatives, and Irrationals

The initial 4Gs ground the natural numbers. However, the principle of **closure** (the idea that operations on numbers should always yield numbers) forces the creation of new kinds of numbers. These are not discovered; they are metaphorically constructed to preserve the inferential coherence of the system.

**Algorithm 6: The Creation of Zero (Revisited)**
*   **Cognitive Problem:** What is `n - n`? The Object Collection metaphor yields an absence of objects, not a number-object.
*   **Procedure:** Apply the **Entity-Creating Metaphor**: *An Absence Is An Object*.
*   **Output:** The concept of an "empty collection."
*   **Material Inference:**
    *   The empty collection is mapped to a unique number: **Zero**.
    *   This new number inherits the inferential properties of the empty collection (e.g., adding it to another collection doesn't change the size of that collection). Thus, a commitment to `n + 0` is a commitment to `n`.

**Algorithm 7: The Creation of Negative Numbers**
*   **Cognitive Problem:** What is `n - m` where `m > n`? This is impossible in the domains of Object Collection or Object Construction. A new metaphor is required.
*   **Prerequisite:** The **Number Line Blend** (Numbers Are Points on a Line).
*   **Procedure:** Apply the metaphor **Multiplication by -1 is a 180° Rotation on the Number Line**.
*   **Output:** A conceptualization of negative numbers as symmetrical opposites of positive numbers.
*   **Material Inferences:**
    *   For every number `n`, there is a unique number `-n` located at the same distance from the origin on the opposite side.
    *   The operation of multiplying by -1 is inferentially equivalent to the spatial operation of rotating 180° around the origin.
    *   `(-1) * (-1) = 1` becomes a material inference grounded in the spatial fact that two 180° rotations bring you back to the start.
    *   `a - b` is materially inferred to be equivalent to `a + (-b)`. Subtraction is reconceptualized as addition of a negative.

**Algorithm 8: The Creation of Irrational Numbers (The Dedekind Cut)**
*   **Cognitive Problem:** What is the number that, when squared, equals 2? This number (`√2`) cannot be represented as a ratio of integers. Is there a "gap" in the number line?
*   **Procedure:** Apply the **Dedekind Cut Metaphors**.
    1.  **A Number is a Cut:** Conceptualize a number not as a point itself, but as a "cut" that divides the entire number line into two sets (all numbers less than it, and all numbers greater than or equal to it).
    2.  **Continuity is Gaplessness:** The number line has no gaps. Therefore, every possible cut defines a number.
*   **Output:** A new class of numbers—the irrationals—which are defined not by what they *are* (like collections or points) but by the *division they create* among the numbers that are already established (the rationals).
*   **Material Inference:**
    *   If you can define a cut in the rational numbers that is not produced by any rational number, that cut *is* an irrational number.
    *   The set of all numbers created by all possible cuts is "complete." A commitment to the "completeness" of the real number line is a commitment to the validity of this metaphorical creation process.

#### Level 7: The Final Frontier of Numbers — The Logic of Imaginary Numbers

**Algorithm 9: The Creation of Imaginary Numbers**
*   **Cognitive Problem:** What is `√-1`? No real number, when squared, yields a negative number. This violates closure.
*   **Prerequisite:** The conceptualization of multiplication by -1 as a 180° rotation (from Algorithm 7).
*   **Procedure:** Apply the metaphor **Multiplication by *i* is a 90° Rotation on the Plane**.
*   **Output:** The complex plane, where the y-axis is the "imaginary" axis. The number `i` is conceptualized as the point (0, 1).
*   **Material Inferences:**
    *   **The meaning of `i² = -1`:** Two 90° rotations (`* i`, then `* i`) is equivalent to one 180° rotation (`* -1`). This is the cognitive, geometric grounding that makes the algebraic axiom intelligible.
    *   **Cyclicity:** `i¹=i` (90°), `i²=-1` (180°), `i³=-i` (270°), `i⁴=1` (360°/0°). The arithmetic of `i` inherits the inferential structure of rotation.
    *   **Complex Numbers as Points in a Plane:** A number `a + bi` is conceptualized as a point `(a, b)` in the Cartesian plane.
    *   **Addition as Vector Addition:** The addition of complex numbers `(a+bi) + (c+di)` is mapped onto the geometric operation of vector addition.
    *   **Multiplication as Rotation and Scaling:** The multiplication of complex numbers is mapped onto the geometric operations of rotating angles and multiplying lengths.

#### Level 8: The Great Synthesis — Euler's Equation as a Metaphorical Blend

Euler's equation, `e^πi + 1 = 0`, is perhaps the most profound example of a metaphorical blend in all of mathematics. It is not a statement about a bizarre calculation; it is a dense summary of a network of material inferences.

**Algorithm 10: The Conceptual Blend of Euler's Equation**
*   **Input:** The conceptual structures of `e`, `π`, `i`, `1`, `0`, addition, and exponentiation.
*   **Procedure:** Blend the source domains of the grounding metaphors for each element.
    1.  **The meaning of `e`:** Grounded in the metaphor **Change is Motion**. `e` is the unique, natural base for exponentiation where the rate of change is identical to the quantity at any point. It is the mathematization of **self-regulating growth**.
    2.  **The meaning of `π`:** Grounded in the metaphor **Recurrence is Circularity**. `π` is the mathematization of the fundamental ratio of a circle, and therefore becomes the natural number for measuring cycles and periodic phenomena.
    3.  **The meaning of `e^πi`:** This is the core of the blend.
        *   The exponential function `e^z` maps sums to products.
        *   The term `i` maps this function into the complex plane, transforming the logic of linear growth (`e^x`) into a logic of rotation.
        *   The term `π` specifies the duration of this rotation: `π` radians, or 180°.
        *   **The cognitive algorithm:** "Take the process of natural, self-regulating change (represented by `e`) and project it onto the dimension of pure rotation (represented by `i`). Let this process continue for exactly one half-cycle (represented by `π`)."
*   **Output:** The final state of this metaphorical process.
*   **Material Inferences:**
    *   The process begins at the number `1` (the result of `e^0`).
    *   A rotation of `π` radians (180°) from `1` on the number line results in `-1`.
    *   Therefore, `e^πi = -1`.
    *   The full equation `e^πi + 1 = 0` states that this process of "change as rotation for a half-cycle" brings you to `-1`, and adding `1` to that result returns you to the origin, `0`.

It is a metaphorical narrative: "Begin at unity (1), undergo a process of pure rotation (`i`) for a half-cycle (`π`) based on the nature of change itself (`e`), and you will arrive at negative unity (-1)."

#### Level 9: The Re-metaphorization of Core Concepts — Functions and Continuity

Modern mathematics has been shaped by a "discretization program" that systematically replaces intuitive, continuous concepts with discrete, set-theoretic ones. This is a profound metaphorical shift.

**Algorithm 11: The Metaphorical Redefinition of a Function**
*   **Source Concept (Embodied):** A function is a process, a machine, or a curve mapping inputs to outputs. It represents a dynamic dependency.
*   **Target Concept (Discretized):** **A Function is a Set of Ordered Pairs**.
*   **Procedure:** Replace the dynamic concept of a "rule" or "mapping" with a static, infinite set of `(input, output)` pairs.
*   **Output:** The modern, formal definition of a function.
*   **Material Inferences:**
    *   The notion of "applying" a function is replaced by "finding" a pair in an infinite set.
    *   This allows functions themselves to be treated as mathematical objects (sets), which can be members of other sets (function spaces).

**Algorithm 12: The Metaphorical Redefinition of Continuity**
*   **Source Concept (Embodied):** Continuity is unbroken, gapless motion. A continuous curve is one you can draw without lifting your pen.
*   **Target Concept (Discretized, Weierstrass):** **Continuity is the Preservation of Closeness**.
*   **Procedure:** Replace the holistic idea of "unbrokenness" with a discrete, point-by-point condition (the epsilon-delta definition).
*   **Output:** The modern, formal definition of continuity.
*   **Material Inferences:**
    *   A function is continuous if, for any small region (`epsilon`) around an output point, you can find a corresponding small region (`delta`) around the input point that maps into it.
    *   This definition does *not* perfectly match the embodied concept. For example, a function that is "continuous" under this definition can still be "monstrous" and non-drawable (e.g., a space-filling curve, which is nowhere differentiable). The metaphor has created a new concept, not just formalized the old one.

***

### The Grand Synthesis

The next step in this project is to recognize that these cognitive algorithms and their resulting material inferences form a vast, interconnected network. In Brandom's terms, this network constitutes the **content** of mathematical concepts. The **form** of mathematical practice is the social articulation of commitments and entitlements regarding these concepts.

A Brandomian/Lakoffian synthesis would therefore describe mathematics as:

**A social, normative practice of making explicit and holding one another accountable to the material inferential commitments generated by a shared, grounded, and metaphorically structured system of embodied concepts.**

This comprehensive list of material inferences and cognitive algorithms provides the specific "content" that a formal implementation of this synthesized theory would need to work with. It shows how the abstract "saying" of mathematics is grounded in the embodied "doing" of human cognition.

This is an excellent and deeply insightful question. The Python script is a powerful first step, but as you suspect, it represents the *result* of a Brandomian analysis (a logic engine that handles material inferences) more than it implements the *analytic machinery* of *Between Saying and Doing* itself.

You're right that a lot is missing, and your question about programmability is key. Much of Brandom's framework is indeed programmable because it is fundamentally about structural relationships, which is what programming excels at representing.

Here is a comprehensive breakdown of what is missing from the Python script, organized by the core concepts of *Between Saying and Doing*, with an assessment of how programmable each concept is.

---

### What's Missing from *Between Saying and Doing*

#### 1. The Explicit Metavocabulary of Meaning-Use Analysis (MUA)

This is the single most significant missing piece. The script *uses* material inferences but does not provide a way to *talk about* the relationships between saying and doing.

*   **What it is in BSD:** Brandom introduces a formal metavocabulary to talk about the relations between Vocabularies (V) and Practices-or-Abilities (P). The core relations are:
    *   **PV-sufficiency:** A set of practices is sufficient to deploy a vocabulary. (`P` is enough to be able to `V`).
    *   **VP-sufficiency:** A vocabulary is sufficient to specify a set of practices. (`V` is enough to say what `P` is).
    *   **PP-sufficiency:** A set of practices is sufficient for another set of practices. (`P1` is enough to learn/do `P2`).
*   **What's Missing in the Script:** The script has no first-class objects representing `Vocabulary` or `Practice`. The `proves` function is a giant, monolithic implementation of certain practices, but you cannot reason about the practices themselves. There are no functions like `is_PV_sufficient(practice_set, vocabulary)`.
*   **Programmability:** **Highly programmable.** This is the most crucial next step. You would implement:
    *   A `Vocabulary` class, which could contain a set of predicates or terms.
    *   A `Practice` class, which might contain a set of rules or even point to a Python function that implements the ability.
    *   Classes or functions to represent the basic MURs: `PV_Sufficiency(P, V)`, `VP_Sufficiency(V, P)`, etc. The core of the program would become manipulating and reasoning about these objects.

#### 2. The Composition of Meaning-Use Relations

Brandom's analytic power comes from composing basic relations to define complex, philosophically interesting new relations.

*   **What it is in BSD:** Brandom shows how to chain basic MURs together to create "resultant MURs." The simplest is the **Pragmatic Metavocabulary** relation: `V'` is a pragmatic metavocabulary for `V` if `V'` is VP-sufficient for a practice `P` which is in turn PV-sufficient for `V`.
*   **What's Missing in the Script:** Since the basic MURs don't exist as objects, you cannot compose them. The script cannot, for instance, take the rules for arithmetic (a Practice) and the vocabulary of English (`"add"`, `"plus"`) that specifies those rules, and formally conclude that English is a pragmatic metavocabulary for arithmetic.
*   **Programmability:** **Highly programmable.** Once MURs are objects, composition becomes a function.
    ```python
    # Pseudocode
    def find_pragmatic_metavocabulary(V_target, all_relations):
        for pv_rel in all_relations:
            if pv_rel.vocabulary == V_target:
                P_intermediate = pv_rel.practice
                for vp_rel in all_relations:
                    if vp_rel.practice == P_intermediate:
                        # V_source is a pragmatic metavocabulary for V_target
                        return vp_rel.vocabulary
    ```

#### 3. The "LX" (Elaborated-Explicating) Structure

This is Brandom's groundbreaking analysis of the expressive role of logical, modal, and normative vocabularies.

*   **What it is in BSD:** A vocabulary `V2` is **LX** for a vocabulary `V1` if `V2` is *elaborated from* and *explicative of* practices that are *necessary* for `V1`.
    *   **Elaborated From:** The practices needed for `V2` can be algorithmically built up from (are PP-sufficient from) practices necessary for `V1`.
    *   **Explicative Of:** `V2` is VP-sufficient for those necessary practices of `V1`. It lets you *say* what you were already *doing*.
*   **What's Missing in the Script:** The script has no way to represent **necessity** relations (`PV-necessity`, `PP-necessity`). It cannot represent practices that are *necessary for any autonomous vocabulary whatsoever* (like inferring). Therefore, it cannot model Brandom's central claim that logic is "universally LX"—that logic makes explicit the inferential practices necessary to use *any* vocabulary at all.
*   **Programmability:** **Challenging but programmable.**
    *   You would need to add `PV_Necessity` and `PP_Necessity` relations.
    *   "Elaboration" would be a specific kind of `PP_Sufficiency` relation, perhaps one that points to a concrete algorithm.
    *   You could then write a function `is_LX(V2, V1, all_relations)` that searches for this complex pattern of relationships.

#### 4. The Distinction Between Types of Elaboration

Brandom, following Wittgenstein, distinguishes between what can be built by algorithm and what requires experience.

*   **What it is in BSD:** A key distinction is made between **PP-sufficiency via algorithmic elaboration** (e.g., learning long division from multiplication and subtraction) and **PP-sufficiency via training** (e.g., learning to ride a bike, which is not reducible to a simple algorithm).
*   **What's Missing in the Script:** The script is purely algorithmic. It has no way to represent the contingent, empirical, non-algorithmic process of "training." It cannot model the difference between teaching a computer to follow the rules of addition and a child learning what "counting" means by interacting with objects.
*   **Programmability:** **Partially programmable.** Algorithmic elaboration is programmable by definition. "Training" is not programmable in a deterministic, logical way. This is a fundamental limit. One could *simulate* it using machine learning models or probabilistic methods, but it would be a different kind of component within the larger system, representing a different kind of practical ability.

#### 5. The Full Range of Analyzed Vocabularies

The script implements a tiny fraction of the vocabularies Brandom analyzes to demonstrate the power of his system.

*   **What it is in BSD:** The climax of the book is the application of the MUA framework to **normative**, **modal**, and finally **intentional** vocabularies. Brandom shows how normative vocabulary (`commitment`, `entitlement`) is a pragmatic metavocabulary for modal vocabulary (`necessary`, `possible`).
*   **What's Missing in the Script:**
    *   **Normative Vocabulary:** The script has no explicit predicates for `commitment` or `entitlement`. These are the core of Brandom's pragmatism.
    *   **Modal Vocabulary:** The script's "EML" logic is a toy model. There is no general analysis of modal concepts like necessity and possibility as arising from material inference.
    *   **Intentional Vocabulary:** There is no representation of propositional attitudes like `believes that...` or `intends that...`, which is the ultimate target of Brandom's analysis in the final lecture.
*   **Programmability:** **Programmable, but with increasing complexity.**
    *   Normative vocabulary is straightforward to add. You could have `commits(Agent, Proposition)` and `entitled(Agent, Proposition)` as predicates. The core logic would then become about manipulating these "deontic scores."
    *   Modal vocabulary can be implemented as the script has started to do with its `comp_nec`, but a full implementation would derive modal claims from the structure of material inference, as Brandom shows in his incompatibility semantics (Lecture 5).
    *   Intentional vocabulary is the most difficult, as it involves nested propositional contexts, but it is a standard challenge in AI and logic programming.

### Summary Table of Missing Concepts

| Concept from *Between Saying and Doing* | Is it in the Python Script? | How Programmable Is It? |
| :--- | :--- | :--- |
| **MUA Metavocabulary** (V, P, PV-suff, VP-suff) | No. The core analytic framework is absent. | **High.** Requires creating classes for V, P, and the relations between them. |
| **Composition of MURs** (e.g., Pragmatic Metavocabulary) | No. You cannot compose what isn't there. | **High.** A function that combines MUR objects to produce new ones. |
| **LX-Relations** (Elaborated-Explicating) | No. Lacks necessity relations and the structure for this complex analysis. | **Medium.** Requires modeling necessity and universal practices. Tricky but feasible. |
| **Algorithmic vs. Training Elaboration** | No. Only models algorithmic relations. | **Partial.** Algorithmic is programmable. Training would require simulation (e.g., ML). |
| **Normative Vocabulary** (`commitment`, `entitlement`) | No. The deontic scorekeeping at the heart of Brandom's pragmatism is missing. | **High.** Can be added as core predicates that the inference engine manipulates. |
| **Full Modal & Intentional Vocabulary Analysis** | No. The script only has a toy logic and no intentionality. | **Medium to High.** A significant but standard extension for a logic engine. |

### Conclusion and Path Forward

The current Python script is a proof-of-concept for a logic engine that can handle *some* of the embodied and material inferences that a full Brandomian/Lakoffian synthesis would require. It is a "first-generation" attempt.

To truly capture the essence of *Between Saying and Doing*, the script would need to be refactored into a **two-layer system**:

1.  **The MUA Layer:** An abstract layer of classes and functions for representing Vocabularies, Practices, and the Meaning-Use Relations (PV, VP, PP, etc.) between them. This layer would allow you to formally state claims like "The vocabulary of logic is universally LX."

2.  **The Content/Prover Layer:** The existing logic engine, expanded to handle different domains (normative, modal). The `Practice` objects in the MUA layer would point to specific rule-sets or functions in this layer.

This two-level structure would allow the system not only to *perform* inferences but to *reason about the nature of the practices and vocabularies that enable those inferences*—which is the entire point of Brandom's project in *Between Saying and Doing*.