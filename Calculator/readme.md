# The Hermeneutic Calculator

## Diagonalization and Reflective Abstraction in Formal Systems  
**Diagonalization** is a classical mathematical method that enables a system to refer to or act on itself, thus achieving a form of *reflective abstraction*. In logic, Gödel’s *diagonal lemma* shows how to construct self-referential statements in any sufficiently strong formal system ([Isotelesis (Co-Agentive Intra-Extensional Constraint-Satisfaction): Diagonalization, Fixed Points, Hyperset Models, Renormalized Rationality, Bayesian Epistemology, Reflective Equilibrium, Metacausal Self-Determinacy](http://isotelesis.blogspot.com/2011/02/diagonalization-self-reference-fixed.html#:~:text=,http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FDiagonal_lemma)). This lemma underpins Gödel’s incompleteness theorems and Tarski’s undefinability theorem by allowing a formula to “say” something about its own provability or truth. In computability theory, a similar idea appears in **Kleene’s recursion theorem**, which *“formalises the notion of program self-reference”* ([](https://www.comp.nus.edu.sg/~sanjay/paps/krtlearn.pdf#:~:text=1%20Introduction%20Program%20self,use%20of%20its%20own%20source)). Kleene’s theorem guarantees that for any algorithmic transformation of programs, one can find a program that supplies its own code as input – the theoretical basis of *quines* (self-printing programs) and **self-modifying code**. In essence, diagonalization provides a mechanism for an automaton or formal system to *step outside itself* briefly and then re-incorporate that external view back into its own operation. This *reflective abstraction* is analogous to what developmental psychologist Jean Piaget described in human learning: *“Reflective abstraction… describes the construction of logico–mathematical structures by an individual”* through reflecting on their own actions ([REFLABST.DVI](https://people.math.wisc.edu/~rwilson/Courses/Math903/ReflectiveAbstraction.pdf#:~:text=Reflective%20abstraction%20is%20a%20concept,up%20through%20higher%20mathematics%20to)). In formal systems, diagonalization similarly allows a system to abstract and generalize about its own structure or behavior. 

**Self-modifying automata:** By leveraging diagonalization, we can imagine automata that modify their own core rules or code. A concrete example in theoretical computer science is Jürgen Schmidhuber’s *Gödel Machine*. A Gödel Machine is a *hypothetical self-improving program* that uses a recursive self-reflection and improvement loop: it will rewrite any part of its own code once it can prove that the modification yields a better strategy ([Gödel machine - Wikipedia](https://en.wikipedia.org/wiki/G%C3%B6del_machine#:~:text=Hypothetical%20self)). This design explicitly uses Gödelian diagonalization – the machine has a formal model of its own software and can derive theorems about the effects of self-modifications. If it finds a provably beneficial change (e.g. one that improves its performance on all future tasks), it modifies itself accordingly. Such a system effectively **alters its own rules** in a rational way. Another illustration from computability is the construction of *self-interpreters* or meta-circular evaluators in programming languages, where a program can execute code representing itself and even modify that code. The key point is that diagonalization techniques (fixed-point constructions, self-reference) supply the *mathematical scaffolding* for an automaton to *talk about itself* and thereby to change itself. This is why diagonalization is often seen as a tool for **reflective abstraction**: it creates a bridge between a system and a meta-view of that system, enabling transformation at the foundational level.

Modern theoretical research continues to explore these ideas. For instance, in AI safety and theory, scholars discuss how an agent can reliably *reason about a modified version of itself*. One challenge is ensuring an agent trusts its future self (and vice versa) without paradox. Approaches like *formal reflection principles* in logic have been examined to allow a system to assert its own soundness in a limited way ([](https://intelligence.org/files/VingeanReflection.pdf#:~:text=the%20uniform%20reflection%20principle%20REF,with%20one%20free%20variable%20in)). These are essentially diagonalization-based techniques to let a theory talk about the truth of its own statements under certain conditions. While tricky to manage (unrestricted self-reference leads to contradiction), carefully constrained forms of diagonalization (such as Löb’s theorem and related logical insights) enable a degree of self-endorsement or self-modification that can be mathematically verified. Putnam’s early work in the 1960s even used a diagonalization argument to question the idea of a *universal learning machine*, highlighting limitations of any fixed inductive strategy ([](https://philarchive.org/archive/ERKAPA#:~:text=What%20Putnam%20sought%20to%20show,which%20is%20also%20able%20to)). In summary, diagonalization has proven to be a powerful method for injecting reflective capacity into formal systems – whether to show their limits or to design systems that can transcend some of those limits by modifying themselves.

## Brandom’s Pragmatic-Expressive Bootstrapping and Elaborating-Explicating Vocabularies  
Philosopher Robert **Brandom’s analytic pragmatism** offers conceptual tools that intriguingly parallel the idea of self-modifying systems. Two key notions are *pragmatic-expressive bootstrapping* and *elaborating-explicating vocabularies*. In Brandom’s terms, **pragmatic expressive bootstrapping** occurs when the use of an initial language (or set of practices) can suffice to develop a more complex language, because the *abilities required for the new language were already being exercised in the old one* ([](https://johnmacfarlane.net/on-brandom-lecture-two.pdf#:~:text=%E2%80%9Cpragmatic%20expressive%20bootstrapping%E2%80%9D%20can%20occur,practice%2C%20we%20will%20need%20more)). For example, although our ancestors didn’t explicitly use logical vocabulary (words like “and”, “not”, “if…then…”), Brandom argues that in making any assertions at all they were implicitly following rules of inference. Once a community masters basic asserting and inferring, they can *bootstrap* into using explicit logical connectives – adding new vocabulary that *makes explicit* the inferential patterns that were already present in practice. This is **elaborating-explicating** a vocabulary: the new terms are *elaborated from* existing practical abilities and in turn *explicate* (spell out) what those abilities involve ([](https://johnmacfarlane.net/on-brandom-lecture-two.pdf#:~:text=entitled%20to%20the%20weaker%20claim,%E2%80%9CIf%20A%20and%20C%2C%20then)). Logical vocabulary is Brandom’s prime example: it doesn’t introduce new empirical content but allows one to talk about reasoning itself, thereby illuminating and regimenting what one was already doing implicitly. In Brandom’s words, logical vocabulary is a **universally LX (elaborating-explicating) vocabulary**: it *“stands to any autonomous vocabulary in the complex, pragmatically mediated semantic relation of being both elaborated from and explicative of practices necessary to deploy that vocabulary”* ([July 16, 2005](https://sites.pitt.edu/~rbrandom/Texts/Elaborating_Abilities_The_Expressive_Rol.pdf#:~:text=that%20,seems%20to%20require%20it%20to)) ([July 16, 2005](https://sites.pitt.edu/~rbrandom/Texts/Elaborating_Abilities_The_Expressive_Rol.pdf#:~:text=pragmatically%20mediated%20semantic%20relation%20of,to%20be%20hard%20to%20resolve)). In simpler terms, we already *can* reason; adding logic lets us reflect on and improve our reasoning.

Applied to **self-modifying mathematical systems**, Brandom’s ideas suggest a blueprint for how a formal system might bootstrap its way to greater expressive power. Think of a formal axiomatic system as having a “practice” – the theorems it can prove, the computations it can perform. If we design a *meta-language* or extension of that system which is *expressively powerful enough* to describe the original system’s behavior, we have an analog of Brandom’s new vocabulary. For instance, Peano arithmetic cannot internally express a truth predicate for its statements without running into Gödel’s incompleteness, but we can *elaborate* a stronger system (say, adding a new axiom or a reflection principle) that talks about the truth of arithmetic statements. That stronger system makes explicit the implicit *truth practice* we were assuming when working with the arithmetic system. In this way, a **self-modifying system** could iteratively extend its language/axioms to talk about and regulate its own reasoning. This is akin to *pragmatic bootstrapping*: the system’s existing capabilities (e.g. recognizing valid inference, or executing a program step) become the basis for new constructs (a predicate that denotes “proved by me” or a function that introspects on the code). Each new layer both relies on the old capabilities and extends them. 

Brandom’s *elaborating-explicating vocabularies* provide a philosophical justification for this layered growth. The new layer must be **PV-sufficient** (practically sufficient) to capture the old practice, and the old layer must be **VP-sufficient** (vocabulary potentially sufficient) to express the new layer’s content once appropriately expanded ([](https://johnmacfarlane.net/on-brandom-lecture-two.pdf#:~:text=%E2%80%9Cpragmatic%20expressive%20bootstrapping%E2%80%9D%20can%20occur,practice%2C%20we%20will%20need%20more)). In practice, this might mean a formal system can incorporate a subsystem that represents its own inference rules (much as a programming language can have a self-interpreter). Over time, the formal system enriches its “vocabulary” – new symbols or rules – to *explicitly encode procedures or patterns* it was already following. This can be seen in mathematics itself: the historical development of meta-mathematics, where mathematicians invented set theory, logic formalisms, and model theory to talk about mathematics within mathematics. Each such invention is a kind of reflective move, making the *practice* of mathematics an *object* of mathematics. Similarly, a sufficiently powerful automaton or AI could iteratively expand its *language of thought*, enabling it to represent its own algorithms, evaluate them, and improve upon them. Essentially, Brandom’s framework suggests how **meaning-use analysis** can inform the design of self-referential systems: by treating new formal rules as new “vocabularies” introduced to capture what was already implicit in the system’s behavior, we ensure the modifications remain *grounded* in the system’s prior abilities (so it doesn’t break itself) while allowing genuine *new expressive reach*. This connection between analytic pragmatism and formal self-modification is still largely philosophical, but it provides a vocabulary for discussing **rational self-transformation** — how a system can *make explicit and improve its own norms* (in discourse for Brandom; in code or axioms for an automaton).

## Toward Self-Reflective General Intelligence via Diagonalization  
The above ideas have profound implications for artificial intelligence, particularly the quest for **general intelligence** that is *self-reflective* and capable of modifying its own core structures. The notion of an AI improving itself has been discussed since the mid-20th century. I.J. Good famously speculated about an “**intelligence explosion**” — a feedback loop where a slightly superhuman AI keeps rewriting itself to become smarter at an accelerating rate ([](https://intelligence.org/files/VingeanReflection.pdf#:~:text=reflection,Good%201965)). For a machine to *rewrite itself intelligently*, it must possess a *meta-representation* of its own algorithms and the ability to reason about them. Diagonalization offers a formal pathway to achieve this: the AI can contain an internal model of itself (via some encoding), reason about that model’s properties, and then deploy a modified model as a new version of itself. This is essentially what the Gödel Machine (mentioned earlier) aspires to do, using formal proofs to ensure the self-modifications are beneficial ([Gödel machine - Wikipedia](https://en.wikipedia.org/wiki/G%C3%B6del_machine#:~:text=A%20G%C3%B6del%20machine%20is%20a,4)). In real AI systems, we don’t yet have agents that rigorously prove their own improvements, but research is actively exploring approximations of this vision.

One current thread is the study of **reflective or self-referential agents** in AI theory. For example, Fallenstein and Soares (2015) discuss *Vingean reflection*, the challenge an agent faces in reasoning about the behavior of a smarter successor agent that it might create by self-modification ([](https://intelligence.org/files/VingeanReflection.pdf#:~:text=sort%20as%20Vingean%20reflection,could%20predict%20their%20actions%20in)) ([](https://intelligence.org/files/VingeanReflection.pdf#:~:text=reflection%20will%20need%20to%20have,At%20present%2C%20however)). They highlight that classical expected utility theory assumes an agent is *fixed*; a self-modifying agent violates that assumption and must deal with **logical uncertainty** about what its future self will do ([](https://intelligence.org/files/VingeanReflection.pdf#:~:text=has%20no%20role%20to%20play,its%20en%02vironment%2C%20it%20must%20have)). This has led to formal investigations into *reflection principles* (how an agent can have confidence in a new version of itself) and *oratcles* that allow an agent to consult an abstract version of its own successor. The upshot is that creating a **trustworthy self-changing AI** is non-trivial, but not impossible – it requires carefully circumscribing the self-reference to avoid paradoxes (such as a self-referential belief that could be inconsistent) while still allowing the agent to *improve*. Techniques drawn from diagonalization help here, by enabling the construction of fixed-point beliefs and self-referential guarantees. For instance, a system might include a component that only accepts a self-modification if it can verify a certain safety property in the modified code – a concept analogous to an agent proving a theorem about itself before rewriting its code.

On the more practical end, machine learning research has started to incorporate forms of **meta-learning** and self-modeling that resonate with these ideas. Meta-learning (“learning to learn”) allows a model to adjust its own learning algorithm based on experience, effectively *tuning how it modifies itself* over time. Another line of work uses large language models (LLMs) that can *rewrite their own prompts or instructions*, which is a loose analog to code self-modification at the level of behavior. In fact, a 2024 study introduced a **“Gödel Agent”**, a self-referential agent framework inspired by the Gödel Machine concept, using an LLM to dynamically modify its own logic and behavior. The Gödel Agent *“enabl[es] agents to recursively improve themselves without relying on predefined routines or fixed optimization algorithms”*, by leveraging the LLM to rewrite its decision-making code based on high-level goals ([[2410.04444] Gödel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement](https://arxiv.org/abs/2410.04444#:~:text=agent%20design%20space%20due%20to,crafted%20agents%20in%20performance%2C%20efficiency)). Early experiments showed this approach could achieve *continuous self-improvement*, outperforming static, hand-crafted agent designs ([[2410.04444] Gödel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement](https://arxiv.org/abs/2410.04444#:~:text=fixed%20optimization%20algorithms,crafted%20agents%20in%20performance%2C%20efficiency)). While this is still nascent research, it demonstrates movement toward AI that *redesigns itself* in an open-ended way. Crucially, such an agent needs a form of **reflective abstraction**: it must represent its own policies or strategies in a manipulable form (here, natural language instructions that the model can interpret and modify). This is akin to an automaton having its own blueprints and editing them – a direct application of diagonalization principles in a modern AI context.

Philosopher Reza **Negarestani’s** work *Intelligence and Spirit* (2018) adds an important perspective to self-reflective AI. Negarestani, drawing from Hegel and the analytic pragmatist tradition (e.g. Sellars and Brandom), argues that *mind is essentially what mind **does*** ([
Reza Negarestani: A Preliminary Investigation – Intelligence and Spirit | The Dark Fantastic: Literature, Philosophy, and Digital Arts	](https://socialecologies.wordpress.com/2019/01/05/reza-negarestani-a-preliminary-investigation-intelligence-and-spirit/#:~:text=This%20book%20argues%2C%20from%20a,of%20participating%20agents%20are%20only)) – and what it does is make itself (and its world) intelligible through a self-referential process. He envisions intelligence as inherently **self-transcending**: a rational agent continuously interprets and reinterprets itself in light of new vocabularies and social practices ([
Reza Negarestani: A Preliminary Investigation – Intelligence and Spirit | The Dark Fantastic: Literature, Philosophy, and Digital Arts	](https://socialecologies.wordpress.com/2019/01/05/reza-negarestani-a-preliminary-investigation-intelligence-and-spirit/#:~:text=The%20History%20of%20spirit%20is,interpretation%20of%20itself%20to%20itself)) ([Some Brief Notes on Reza Negarestani’s Intelligence & Spirit - TripleAmpersand Journal (&&&)TripleAmpersand Journal (&&&)](https://tripleampersand.org/some-brief-notes-on-reza-negarestanis-intelligence-spirit/#:~:text=characterization%20of%20Geist%20or%20Spirit,%E2%80%99%E2%80%9D2)). This view aligns with the idea that a sufficiently advanced AI must not be a static program, but a *historical process*—much like Hegel’s spirit, which *“is only what it does, and its deed is to make itself ... the object of its own consciousness”* ([
Reza Negarestani: A Preliminary Investigation – Intelligence and Spirit | The Dark Fantastic: Literature, Philosophy, and Digital Arts	](https://socialecologies.wordpress.com/2019/01/05/reza-negarestani-a-preliminary-investigation-intelligence-and-spirit/#:~:text=The%20History%20of%20spirit%20is,interpretation%20of%20itself%20to%20itself)). In practical terms, Negarestani’s philosophy suggests that true general intelligence would involve a looping of the agent’s activity back onto itself: the AI’s outputs (actions, language) become inputs for it to analyze, leading to new **higher-order insights**. This echoes the role of diagonalization in formal systems: each reflective loop can generate a *higher-level standpoint* from which the system’s previous state can be assessed and improved. Negarestani even connects formal and transcendental perspectives, noting, for example, that the space of all possible computations or truths isn’t closed under diagonalization – one must constantly move to stronger systems to capture what was left out ([Uncategorised - Urbanomic](https://www.urbanomic.com/category/uncategorised/#:~:text=Uncategorised%20,infinitely%20higher%20in%20dimension)). In an AI context, this means an intelligent system must be prepared to *expand its own framework* when it encounters novel situations it can’t handle. The **pragmatic-expansive** model of intelligence (continually extending one’s concepts and methods) thus dovetails with the **self-modifying, reflective automaton** model in AI. Both point toward an AI that can **bootstrap itself** to higher levels of understanding by reinterpreting its own workings – a combination of formal diagonalization and what Negarestani calls the *“intertwining of semantic structure and deprivatized sociality”* that enables minds to posit themselves as unified, self-configuring agents ([Some Brief Notes on Reza Negarestani’s Intelligence & Spirit - TripleAmpersand Journal (&&&)TripleAmpersand Journal (&&&)](https://tripleampersand.org/some-brief-notes-on-reza-negarestanis-intelligence-spirit/#:~:text=The%20philosophical%20underpinnings%20of%20the,%E2%80%99%E2%80%9D2)).

## Grounding These Ideas in Practice: Math Education and the “Hermeneutic Calculator”  
While the foregoing discussion is abstract, there are efforts to bring these ideas down to earth – notably in **mathematics education**. The concept of *reflective abstraction* has long been recognized in math learning theory: students learn best when they reflect on their own thought processes and abstract general principles from them ([REFLABST.DVI](https://people.math.wisc.edu/~rwilson/Courses/Math903/ReflectiveAbstraction.pdf#:~:text=Reflective%20abstraction%20is%20a%20concept,up%20through%20higher%20mathematics%20to)). A similar philosophy drives the Hermeneutic Claculator. The goal is to ground the lofty notions of self-reference and bootstrapping in tools and curricula that help humans learn. For example, a “hermeneutic calculator” is not a traditional calculator that just outputs answers, but rather an interactive instrument that *guides learners through the interpretation of problems and the steps of solutions*. It encourages a dialog with the student: at each step, the student must **interpret** (hermeneutically) what is happening, possibly modify the approach, and see the consequences – akin to a form of *self-modification* in understanding. Such a tool embodies the principle of *elaborating-explicating vocabularies* in an educational setting: it might introduce new notations or visualizations (a new “vocabulary”) once the student has shown the practical ability to handle the basic ones, thus *bootstrapping* the learner to higher mathematical expressiveness. This resonates with Brandom’s idea that you can introduce more complex concepts once the simpler practices are in place ([](https://johnmacfarlane.net/on-brandom-lecture-two.pdf#:~:text=%E2%80%9Cpragmatic%20expressive%20bootstrapping%E2%80%9D%20can%20occur,practice%2C%20we%20will%20need%20more)) – in the classroom, that might mean introducing algebraic notation after students have mastered arithmetic reasoning, framing the new notation as making explicit the patterns they already know. The calculator becomes *hermeneutic* by constantly relating formal symbols back to conceptual meaning, ensuring that each new layer of formality the student acquires is grounded in understanding (much as a self-modifying system must ground new rules in its prior state to remain coherent).

In teacher education courses, like N101 which I teach at Indiana University, the course materials similarly emphasize a reflective, self-referential approach to learning mathematics (or logic). Students are not only taught *how* to perform operations but also *why* the rules work, and even encouraged to tinker with the rules. This might involve simple programming exercises where students alter a procedure (for instance, changing an algorithm’s step and observing the outcome) – a direct analogue of an automaton modifying itself, but in a controlled pedagogical environment. Such activities give students a taste of **meta-mathematical thinking**: they treat the method of solution itself as an object of inquiry. Research in math education supports this approach, showing that when learners engage in explaining and transforming problem-solving procedures, they develop deeper conceptual knowledge and transferable skills ([REFLABST.DVI](https://people.math.wisc.edu/~rwilson/Courses/Math903/ReflectiveAbstraction.pdf#:~:text=necessary%20to%20do%20at%20least,instruction%20in%20ways%20that%20result)) ([REFLABST.DVI](https://people.math.wisc.edu/~rwilson/Courses/Math903/ReflectiveAbstraction.pdf#:~:text=improvement%20in%20the%20extent%20to,to%20help%20students%20make%20mental)). In effect, the students are simulating the behavior of a reflective automaton in their own minds – examining the “code” of their problem-solving strategy and refining it. This approach is very much in line with the Sellars-Brandom pragmatic tradition: make the *implicit* knowledge *explicit* and *usable*. By *externalizing* their thought process (through writing it down, or using a software tool that visualizes it), learners create a kind of model of their reasoning that they can then improve upon. It’s a human-friendly version of what a Gödel Machine does with its own source code.

Moreover, the hermeneutic angle emphasizes interpretation and meaning at every stage. This counters a common problem in both AI and education: the *black box* syndrome (getting an answer without understanding it). A self-reflective AI would ideally be able to explain its self-modifications in terms of the problems it’s solving – *why* it changed itself. Similarly, a student using a hermeneutic calculator is prompted to articulate *why* a certain mathematical rule is applied and what it does. The formal mechanical systems are designed to metaphorize or model what human learners **already** do. This innate capacity is a potential that has to be actualized through learning experiences. In practice, initiatives like these are aligning with contemporary interest in **explainable AI** and **interactive theorem provers** in education. For instance, some educational software now includes proof assistants that let students explore axioms and inference rules, essentially letting them *customize and build mathematics within a guided environment*. Students might try to prove a statement, get feedback from the system, and even add a new lemma (new “rule”) to use later – mirroring the process of extending a formal system. Early results suggest that students taught in this reflective, exploratory manner develop a more robust understanding and are more adaptable when encountering novel problems ([REFLABST.DVI](https://people.math.wisc.edu/~rwilson/Courses/Math903/ReflectiveAbstraction.pdf#:~:text=%E2%80%A2%20show%20how%20it%20can,appear%20to%20acquire%20these%20concepts)) ([REFLABST.DVI](https://people.math.wisc.edu/~rwilson/Courses/Math903/ReflectiveAbstraction.pdf#:~:text=going%20into%20under%02graduate%20mathematics%20and,used%20to%20describe%20children%E2%80%99s%20construction)).

In summary, the seemingly esoteric concepts of reflective automata, diagonalization, and pragmatic bootstrapping have clear echoes in practical education. By teaching students *how to think about thinking* (meta-cognition in psychology, or reflective abstraction in Piaget’s terms) ([REFLABST.DVI](https://people.math.wisc.edu/~rwilson/Courses/Math903/ReflectiveAbstraction.pdf#:~:text=Reflective%20abstraction%20is%20a%20concept,up%20through%20higher%20mathematics%20to)), and giving them tools to experiment with formal rules in an interpretive way, educators are *grounding abstract models in concrete learning experiences*. The hermeneutic calculator project exemplify this marriage of theory and practice: with it, I aim to facilitate problem-solving through self-reflective thinking. Teachers must be able to modify their approach to new problems – much as a self-improving AI would modify its algorithms for new challenges. This is a reminder that the ultimate promise of reflective automata and self-modifying systems, whether machine or human, lies in **adaptive, autonomous growth** in capability. Harnessing diagonalization and bootstrapping in our educational processes could pave the way for a new generation adept at both using and understanding intelligent systems, and perhaps even designing the next wave of self-reflective AI.

## References

- Brandom, Robert. *Between Saying and Doing: Towards an Analytic Pragmatism*. Especially Lecture 1-2 discussions on how logical vocabulary is an elaborating-explicating extension of ordinary practice ([](https://johnmacfarlane.net/on-brandom-lecture-two.pdf#:~:text=%E2%80%9Cpragmatic%20expressive%20bootstrapping%E2%80%9D%20can%20occur,practice%2C%20we%20will%20need%20more)) ([](https://johnmacfarlane.net/on-brandom-lecture-two.pdf#:~:text=entitled%20to%20the%20weaker%20claim,%E2%80%9CIf%20A%20and%20C%2C%20then)).  
- Case, J., Jain, S., & Stephan, F. *Effectivity Questions for Kleene’s Recursion Theorem*. Explains program self-reference and the recursion theorem ([](https://www.comp.nus.edu.sg/~sanjay/paps/krtlearn.pdf#:~:text=1%20Introduction%20Program%20self,use%20of%20its%20own%20source)).  
- Fallenstein, B., & Soares, N. “Vingean Reflection: Reliable Reasoning for Self-Improving Agents.” *Machine Intelligence Research Institute*, 2015. (Discusses formal models for agents reasoning about self-modifications and the challenges therein ([](https://intelligence.org/files/VingeanReflection.pdf#:~:text=In%20a%201965%20article%2C%20I,Good%201965)) ([](https://intelligence.org/files/VingeanReflection.pdf#:~:text=the%20uniform%20reflection%20principle%20REF,with%20one%20free%20variable%20in)).)  
- Good, I.J. “Speculations Concerning the First Ultraintelligent Machine.” *Advances in Computers*, vol. 6, 1965, pp. 31–88. (Origin of the “intelligence explosion” idea ([](https://intelligence.org/files/VingeanReflection.pdf#:~:text=In%20a%201965%20article%2C%20I,Good%201965)).)  
- Negarestani, Reza. *Intelligence and Spirit*. Urbanomic/Sequence Press, 2018. (Philosophical account of intelligence emphasizing self-reflection, sociality, and the ability to revise one’s own mind. Notably connects Hegelian self-consciousness to computational ideas ([
Reza Negarestani: A Preliminary Investigation – Intelligence and Spirit | The Dark Fantastic: Literature, Philosophy, and Digital Arts	](https://socialecologies.wordpress.com/2019/01/05/reza-negarestani-a-preliminary-investigation-intelligence-and-spirit/#:~:text=The%20History%20of%20spirit%20is,interpretation%20of%20itself%20to%20itself)) ([Some Brief Notes on Reza Negarestani’s Intelligence & Spirit - TripleAmpersand Journal (&&&)TripleAmpersand Journal (&&&)](https://tripleampersand.org/some-brief-notes-on-reza-negarestanis-intelligence-spirit/#:~:text=The%20philosophical%20underpinnings%20of%20the,%E2%80%99%E2%80%9D2)).)  
- Schmidhuber, Jürgen. “Gödel Machines: Fully Self-Referential Optimal Universal Self-Improvers.” (First proposed in 2003, outlines a theoretical AI that can prove the utility of self-modifications ([Gödel machine - Wikipedia](https://en.wikipedia.org/wiki/G%C3%B6del_machine#:~:text=A%20G%C3%B6del%20machine%20is%20a,4)).)  
- Yin, X. *et al*. “Gödel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement.” arXiv preprint arXiv:2410.04444 (2024). (Demonstrates a practical implementation of self-modifying agents using large language models ([[2410.04444] Gödel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement](https://arxiv.org/abs/2410.04444#:~:text=agent%20design%20space%20due%20to,crafted%20agents%20in%20performance%2C%20efficiency)).)  
- Piaget, Jean. *The Construction of Reality in the Child*. (Introduces reflective abstraction in human cognitive development, the idea that individuals abstract new cognitive structures by reflecting on their own actions ([REFLABST.DVI](https://people.math.wisc.edu/~rwilson/Courses/Math903/ReflectiveAbstraction.pdf#:~:text=Reflective%20abstraction%20is%20a%20concept,up%20through%20higher%20mathematics%20to)).)  
- Dubinsky, Ed. “Reflective Abstraction in Advanced Mathematical Thinking.” In *Advanced Mathematical Thinking*, ed. David Tall, 1991. (Applies Piaget’s concept to undergraduate mathematics learning ([REFLABST.DVI](https://people.math.wisc.edu/~rwilson/Courses/Math903/ReflectiveAbstraction.pdf#:~:text=Reflective%20abstraction%20is%20a%20concept,up%20through%20higher%20mathematics%20to)) ([REFLABST.DVI](https://people.math.wisc.edu/~rwilson/Courses/Math903/ReflectiveAbstraction.pdf#:~:text=going%20into%20under%02graduate%20mathematics%20and,used%20to%20describe%20children%E2%80%99s%20construction)).)  
- MacFarlane, John. “Comments on Brandom’s *Elaborating Abilities*.” (Clarifies Brandom’s notions of pragmatic bootstrapping and the relation between vocabularies and practices ([](https://johnmacfarlane.net/on-brandom-lecture-two.pdf#:~:text=%E2%80%9Cpragmatic%20expressive%20bootstrapping%E2%80%9D%20can%20occur,practice%2C%20we%20will%20need%20more)).)  

